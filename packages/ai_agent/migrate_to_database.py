"""
æ•°æ®è¿ç§»è„šæœ¬ï¼šå°† complete_defi_data.json å¯¼å…¥ PostgreSQL
"""
import json
import os
import sys
from datetime import datetime
from typing import List, Dict
import logging
from dotenv import load_dotenv

from packages.ai_agent.database import DatabaseManager

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

load_dotenv()


class DataMigrator:
    """æ•°æ®è¿ç§»å·¥å…·"""
    
    def __init__(self, json_file_path: str = 'data/complete_defi_data.json'):
        self.json_file_path = json_file_path
        self.db = DatabaseManager()
        self.stats = {
            'total_snapshots': 0,
            'successful_snapshots': 0,
            'failed_snapshots': 0,
            'pools_processed': 0
        }
    
    def load_json_data(self) -> Dict:
        """åŠ è½½JSONæ–‡ä»¶"""
        if not os.path.exists(self.json_file_path):
            raise FileNotFoundError(
                f"âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨: {self.json_file_path}\n"
                f"è¯·å…ˆè¿è¡Œ data_fetcher.py è·å–æ•°æ®"
            )
        
        logger.info(f"ğŸ“‚ åŠ è½½æ•°æ®: {self.json_file_path}")
        with open(self.json_file_path, 'r') as f:
            data = json.load(f)
        
        pools_count = len(data.get('pools', {}))
        logger.info(f"âœ… æ‰¾åˆ° {pools_count} ä¸ªæ± å­çš„æ•°æ®")
        
        return data
    
    def transform_snapshot(
        self, 
        pool_symbol: str,
        raw_snap: Dict,
        aave_apy: float,
        gas_cost: float
    ) -> Dict:
        """
        è½¬æ¢å•ä¸ªå¿«ç…§ä¸ºæ•°æ®åº“æ ¼å¼
        
        Args:
            pool_symbol: æ± å­ç¬¦å·
            raw_snap: åŸå§‹å¿«ç…§æ•°æ®
            aave_apy: Aave APY (å°æ—¶åŒ–)
            gas_cost: Gasè´¹ç”¨ (USD)
        
        Returns:
            è½¬æ¢åçš„å¿«ç…§
        """
        try:
            # è§£ææ—¶é—´æˆ³
            timestamp = datetime.fromtimestamp(int(raw_snap['periodStartUnix']))
            
            # æå–ä»·æ ¼å’Œäº¤æ˜“æ•°æ®
            wbtc_price = float(raw_snap.get('token0Price', 0))
            volume_usd = float(raw_snap.get('volumeUSD', 0))
            liquidity = float(raw_snap.get('liquidity', 0))
            tvl_usd = float(raw_snap.get('tvlUSD', 1))
            
            # è®¡ç®—UniV3 LP APY (åŸºäºæ‰‹ç»­è´¹æ”¶å…¥)
            # APY â‰ˆ (volume / TVL) * fee_rate
            fee_rate = 0.003  # 0.3% æ‰‹ç»­è´¹
            univ3_lp_apy = (volume_usd / tvl_usd) * fee_rate if tvl_usd > 0 else 0
            
            # é™åˆ¶APYåœ¨åˆç†èŒƒå›´ (0-1% æ¯å°æ—¶)
            univ3_lp_apy = max(0, min(univ3_lp_apy, 0.01))
            
            return {
                'pool_symbol': pool_symbol,
                'timestamp': timestamp,
                'wbtc_price': wbtc_price,
                'volume_usd': volume_usd,
                'liquidity': liquidity,
                'tvl_usd': tvl_usd,
                'aave_wbtc_apy': aave_apy,
                'univ3_lp_apy': univ3_lp_apy,
                'gas_cost_usd': gas_cost
            }
        
        except Exception as e:
            logger.warning(f"âš ï¸  è·³è¿‡å¼‚å¸¸å¿«ç…§: {e}")
            return None
    
    def migrate_pool_snapshots(self, pool_symbol: str, pool_data: Dict):
        """è¿ç§»æ± å­çš„å¿«ç…§æ•°æ®"""
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“Š å¤„ç†æ± å­: {pool_symbol}")
        logger.info(f"{'='*70}")
        
        # è·å–Aave APY
        aave_reserves = pool_data.get('aave_current_reserves', [])
        aave_wbtc_apy = 0.0
        
        for reserve in aave_reserves:
            if reserve.get('symbol') == 'WBTC':
                # liquidityRate æ˜¯å¹´åŒ–åˆ©ç‡ (Rayæ ¼å¼: 1e27)
                annual_rate = float(reserve.get('liquidityRate', 0)) / 1e27
                # è½¬æ¢ä¸ºå°æ—¶APY
                aave_wbtc_apy = annual_rate / (24 * 365)
                logger.info(f"  ğŸ“ˆ Aave WBTC APY: {aave_wbtc_apy*100*24*365:.2f}% å¹´åŒ–")
                break
        
        # è·å–Gasè´¹ç”¨
        gas_data = pool_data.get('gas_current', {})
        gas_cost_usd = gas_data.get('base_fee_gwei', 0.001) * 0.01  # ä¼°ç®—
        
        # è½¬æ¢æ‰€æœ‰å¿«ç…§
        raw_snapshots = pool_data.get('snapshots', [])
        logger.info(f"  ğŸ“¦ åŸå§‹å¿«ç…§æ•°: {len(raw_snapshots)}")
        
        transformed_snapshots = []
        for raw_snap in raw_snapshots:
            snapshot = self.transform_snapshot(
                pool_symbol, 
                raw_snap, 
                aave_wbtc_apy, 
                gas_cost_usd
            )
            if snapshot:
                transformed_snapshots.append(snapshot)
        
        logger.info(f"  âœ… æœ‰æ•ˆå¿«ç…§æ•°: {len(transformed_snapshots)}")
        
        if not transformed_snapshots:
            logger.warning(f"  âš ï¸  æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
            return
        
        # æ‰¹é‡æ’å…¥ (æ¯æ¬¡500æ¡ï¼Œé¿å…è¶…æ—¶)
        batch_size = 500
        total_inserted = 0
        
        for i in range(0, len(transformed_snapshots), batch_size):
            batch = transformed_snapshots[i:i + batch_size]
            
            try:
                inserted = self.db.insert_pool_snapshots(batch)
                total_inserted += inserted
                
                progress = (i + len(batch)) / len(transformed_snapshots) * 100
                logger.info(
                    f"  ğŸ’¾ æ‰¹æ¬¡ {i//batch_size + 1}: "
                    f"æ’å…¥ {len(batch)} æ¡ "
                    f"({progress:.1f}%)"
                )
                
            except Exception as e:
                logger.error(f"  âŒ æ‰¹æ¬¡æ’å…¥å¤±è´¥: {e}")
                self.stats['failed_snapshots'] += len(batch)
                continue
        
        self.stats['successful_snapshots'] += total_inserted
        self.stats['pools_processed'] += 1
        
        logger.info(f"  âœ… æ± å­å¤„ç†å®Œæˆï¼Œå…±æ’å…¥ {total_inserted} æ¡è®°å½•")
    
    def migrate_strategy_logs(self):
        """è¿ç§»ç­–ç•¥æ‰§è¡Œæ—¥å¿—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
        log_file = 'logs/strategy_executions.jsonl'
        
        if not os.path.exists(log_file):
            logger.info("\nğŸ“ æœªæ‰¾åˆ°ç­–ç•¥æ—¥å¿—æ–‡ä»¶ï¼Œè·³è¿‡...")
            return
        
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ“ è¿ç§»ç­–ç•¥æ‰§è¡Œæ—¥å¿—")
        logger.info(f"{'='*70}")
        
        count = 0
        errors = 0
        
        with open(log_file, 'r') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    log_entry = json.loads(line.strip())
                    
                    # å‡†å¤‡æ‰§è¡Œè®°å½•
                    execution = {
                        'pool_symbol': log_entry.get('pool_symbol', 'wBTC-USDC'),
                        'timestamp': log_entry.get('timestamp'),
                        'aave_wbtc_pool': float(log_entry.get('aave_wbtc_pool', 0)),
                        'uniswap_v3_lp': float(log_entry.get('uniswap_v3_lp', 0)),
                        'tx_hash': log_entry.get('tx_hash'),
                        'model_confidence': log_entry.get('model_confidence'),
                        'safety_bounds': log_entry.get('safety_bounds'),
                        'additional_info': {
                            k: v for k, v in log_entry.items() 
                            if k not in ['pool_symbol', 'timestamp', 'aave_wbtc_pool', 
                                       'uniswap_v3_lp', 'tx_hash', 'model_confidence', 
                                       'safety_bounds']
                        }
                    }
                    
                    self.db.insert_strategy_execution(execution)
                    count += 1
                    
                    if count % 10 == 0:
                        logger.info(f"  ğŸ’¾ å·²è¿ç§» {count} æ¡æ—¥å¿—...")
                    
                except Exception as e:
                    errors += 1
                    logger.warning(f"  âš ï¸  ç¬¬{line_num}è¡Œè§£æå¤±è´¥: {e}")
                    continue
        
        logger.info(f"  âœ… ç­–ç•¥æ—¥å¿—è¿ç§»å®Œæˆ: {count} æˆåŠŸ, {errors} å¤±è´¥")
    
    def verify_migration(self):
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info(f"\n{'='*70}")
        logger.info("ğŸ” éªŒè¯è¿ç§»ç»“æœ")
        logger.info(f"{'='*70}")
        
        try:
            stats = self.db.get_database_stats()
            
            # æ˜¾ç¤ºå¿«ç…§ç»Ÿè®¡
            logger.info("\nğŸ“Š æ± å­å¿«ç…§ç»Ÿè®¡:")
            for pool_stat in stats['snapshots']:
                pool = pool_stat['pool_symbol']
                count = pool_stat['count']
                earliest = pool_stat['earliest']
                latest = pool_stat['latest']
                
                logger.info(f"  â€¢ {pool}: {count:,} æ¡è®°å½•")
                logger.info(f"    æ—¶é—´èŒƒå›´: {earliest} è‡³ {latest}")
            
            # æ˜¾ç¤ºç­–ç•¥ç»Ÿè®¡
            if stats['strategies']:
                logger.info("\nğŸ¯ ç­–ç•¥æ‰§è¡Œç»Ÿè®¡:")
                for strat_stat in stats['strategies']:
                    pool = strat_stat['pool_symbol']
                    count = strat_stat['count']
                    logger.info(f"  â€¢ {pool}: {count} æ¡è®°å½•")
            else:
                logger.info("\nğŸ¯ ç­–ç•¥æ‰§è¡Œç»Ÿè®¡: æš‚æ— æ•°æ®")
            
            # æ€»ç»“
            total_snapshots = sum(s['count'] for s in stats['snapshots'])
            total_strategies = sum(s['count'] for s in stats['strategies'])
            
            logger.info(f"\nğŸ“ˆ æ€»è®¡:")
            logger.info(f"  â€¢ å¿«ç…§æ€»æ•°: {total_snapshots:,}")
            logger.info(f"  â€¢ ç­–ç•¥è®°å½•: {total_strategies}")
            
        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
    
    def run_full_migration(self):
        """è¿è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        logger.info("\n" + "="*70)
        logger.info("ğŸš€ å¼€å§‹æ•°æ®è¿ç§»åˆ° PostgreSQL (Supabase)")
        logger.info("="*70)
        
        start_time = datetime.now()
        
        try:
            # 1. åŠ è½½JSONæ•°æ®
            json_data = self.load_json_data()
            
            # 2. è¿ç§»æ¯ä¸ªæ± å­çš„å¿«ç…§
            pools = json_data.get('pools', {})
            
            if not pools:
                logger.error("âŒ JSONä¸­æ²¡æœ‰æ‰¾åˆ°æ± å­æ•°æ®")
                return
            
            self.stats['total_snapshots'] = sum(
                len(pool_data.get('snapshots', [])) 
                for pool_data in pools.values()
            )
            
            for pool_symbol, pool_data in pools.items():
                self.migrate_pool_snapshots(pool_symbol, pool_data)
            
            # 3. è¿ç§»ç­–ç•¥æ—¥å¿—
            self.migrate_strategy_logs()
            
            # 4. éªŒè¯ç»“æœ
            self.verify_migration()
            
            # 5. æ˜¾ç¤ºæ€»ç»“
            elapsed = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"\n{'='*70}")
            logger.info("âœ… è¿ç§»å®Œæˆï¼")
            logger.info(f"{'='*70}")
            logger.info(f"â±ï¸  è€—æ—¶: {elapsed:.1f} ç§’")
            logger.info(f"ğŸ“Š ç»Ÿè®¡:")
            logger.info(f"  â€¢ å¤„ç†æ± å­æ•°: {self.stats['pools_processed']}")
            logger.info(f"  â€¢ å¿«ç…§æ€»æ•°: {self.stats['total_snapshots']:,}")
            logger.info(f"  â€¢ æˆåŠŸæ’å…¥: {self.stats['successful_snapshots']:,}")
            logger.info(f"  â€¢ å¤±è´¥: {self.stats['failed_snapshots']}")
            
            logger.info(f"\nğŸ“š ä¸‹ä¸€æ­¥:")
            logger.info(f"  1. å¯åŠ¨ Analytics API: python analytics_api.py")
            logger.info(f"  2. æµ‹è¯•æ¥å£: curl http://localhost:8001/api/v1/analytics/health")
            logger.info(f"  3. ä¿®æ”¹ agent.py å¯ç”¨æ•°æ®åº“æ—¥å¿—")
            
        except Exception as e:
            logger.error(f"\nâŒ è¿ç§»å¤±è´¥: {e}", exc_info=True)
            sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='å°† DeFi æ•°æ®ä» JSON è¿ç§»åˆ° PostgreSQL'
    )
    parser.add_argument(
        '--json-file',
        default='data/complete_defi_data.json',
        help='JSON æ•°æ®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--verify-only',
        action='store_true',
        help='ä»…éªŒè¯ç°æœ‰æ•°æ®ï¼Œä¸æ‰§è¡Œè¿ç§»'
    )
    
    args = parser.parse_args()
    
    migrator = DataMigrator(json_file_path=args.json_file)
    
    if args.verify_only:
        migrator.verify_migration()
    else:
        migrator.run_full_migration()


if __name__ == "__main__":
    main()